{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Purpose of this file is to get phrases from the Textual data. This file will take some amount of time to execute. So preferably execute this file on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "trSlIMT_U8B1",
    "outputId": "20d62a4e-d905-4bd3-cd4a-edb82942e2c4"
   },
   "outputs": [],
   "source": [
    "#Generic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "#NLP Libraries\n",
    "import nltk\n",
    "#Model Libraries\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "xxobNld2eYlU",
    "outputId": "f75bba2c-0417-47af-d3c5-ee0bee5e87e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1057 entries, 0 to 1056\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   bio                 1057 non-null   object\n",
      " 1   word_tokenized      1057 non-null   object\n",
      " 2   sentence_tokenized  1057 non-null   object\n",
      " 3   word_count          1057 non-null   int64 \n",
      " 4   sentence_count      1057 non-null   int64 \n",
      " 5   clean_words         1057 non-null   object\n",
      " 6   clean_stemmed       1057 non-null   object\n",
      " 7   clean_lemmed        1057 non-null   object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 66.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('data\\df_processed_bio.csv')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "mrZ8od8-AVFe",
    "outputId": "72ba44e3-0c83-4224-a846-1d4327cd97e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1053, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duplicates reduce the df by 651 observations\n",
    "df2.drop_duplicates('bio', inplace= True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMz98Ii7FhO2"
   },
   "source": [
    "# POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84nC3dhEePhF"
   },
   "outputs": [],
   "source": [
    "def pos_series(keyword):\n",
    "    '''categorizes after tokenizing words with POS tags'''\n",
    "    tokens = nltk.word_tokenize(keyword)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ha3ngJ7NeP4v"
   },
   "outputs": [],
   "source": [
    "# cell runs slower due to computational exhaustion; gpu not active\n",
    "pos_tagged_arrs = df2.bio.apply(pos_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMjUNxnReQT4"
   },
   "outputs": [],
   "source": [
    "# unloads the tuples from the tree object for easier manipulation\n",
    "pos_tagged = []\n",
    "for row in pos_tagged_arrs.values:\n",
    "    for element in row:\n",
    "        pos_tagged.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PM4QXbWNeviQ"
   },
   "outputs": [],
   "source": [
    "# dataframe contains all of the words with their corresponding pos tag;\n",
    "pos_df = pd.DataFrame(pos_tagged, columns = ('word','POS'))\n",
    "# special chars were removed due to irrelevance as a tag but will be included in regex\n",
    "char_removal = [',', '.', ':', '#', '$', '\\'\\'', '``', '(', ')','@']\n",
    "drop_indices = (pos_df.loc[pos_df.POS.isin(char_removal)].index)\n",
    "pos_df.drop(drop_indices, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeEReq61HT1r"
   },
   "source": [
    "# Noun Prase #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1n4F_64VS7q"
   },
   "outputs": [],
   "source": [
    "# defining grammer within the text using reg expressions\n",
    "# optional determinate, any number of adjectives, a noun, noun plural, proper noun with additionals following\n",
    "grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "chunkParser = nltk.RegexpParser(grammar1)\n",
    "tree1 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhQm0Y6EYG8T"
   },
   "outputs": [],
   "source": [
    "# typical noun phrase pattern to be pickled for later analyses\n",
    "g1_chunks = []\n",
    "for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "    # print(subtree)\n",
    "    g1_chunks.append(subtree)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mqb7BCe2mr95"
   },
   "outputs": [],
   "source": [
    "with open('chunks_bio_1.pickle', 'wb') as fp1:\n",
    "    pickle.dump(g1_chunks, fp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8AsEdg8Hdpo"
   },
   "source": [
    "# Noun Phrase #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7AuWWmCih7w"
   },
   "outputs": [],
   "source": [
    "# Noun phrase variation \n",
    "# preposition maybe, any number of adjective or nouns, any plural nouns or singular nouns\n",
    "grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "chunkParser = nltk.RegexpParser(grammar2)\n",
    "tree2 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZtufY7eYKwJ"
   },
   "outputs": [],
   "source": [
    "# variation of a noun phrase pattern to be pickled for later analyses\n",
    "g2_chunks = []\n",
    "for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "    # print(subtree)\n",
    "    g2_chunks.append(subtree)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQ8ApP1fmyod"
   },
   "outputs": [],
   "source": [
    "with open('chunks_bio_2.pickle', 'wb') as fp2:\n",
    "    pickle.dump(g2_chunks , fp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUPMERSrHhiF"
   },
   "source": [
    "Verb-Noun Pattern #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLkcIxO0VTO9"
   },
   "outputs": [],
   "source": [
    "# any sort of verb followed by any number of nouns\n",
    "grammar3 = ('''\n",
    "    VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}\n",
    "    ''')\n",
    "chunkParser = nltk.RegexpParser(grammar3)\n",
    "tree3 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pR3EFTcAjN1C"
   },
   "outputs": [],
   "source": [
    "# verb-noun pattern to be pickled for later analyses\n",
    "g3_chunks = []\n",
    "for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "    # print(subtree)\n",
    "    g3_chunks.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UooPLPB0m6p_"
   },
   "outputs": [],
   "source": [
    "with open('chunks_bio_3.pickle', 'wb') as fp3:\n",
    "    pickle.dump(g3_chunks, fp3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebFXJ-PQHmqh"
   },
   "source": [
    "# <noun, noun*> Pattern #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmrbtjenVTfR"
   },
   "outputs": [],
   "source": [
    "# any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "grammar4 = ('''\n",
    "    Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} \n",
    "    ''')\n",
    "chunkParser = nltk.RegexpParser(grammar4)\n",
    "tree4 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WOwZEb8kHLl"
   },
   "outputs": [],
   "source": [
    "# common pattern of listing skills to be pickled for later analyses\n",
    "g4_chunks = []\n",
    "for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "    # print(subtree)\n",
    "    g4_chunks.append(subtree)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Krjr8UmgnYnS"
   },
   "outputs": [],
   "source": [
    "with open('chunks_bio_4.pickle', 'wb') as fp4:\n",
    "    pickle.dump(g4_chunks, fp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Job Descriptions_POS_and_Chunking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
